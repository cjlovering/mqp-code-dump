{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coalese data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import nltk\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional symbol that could be mixed into the text\n",
    "additional = [\n",
    "    '!','[',']','{', '}','-', '(',')',\"'\",'\"','!','%','$','#',':',';','.',',','\\n','?','/','@','`','~'\n",
    "]\n",
    "def clean_data(data):\n",
    "    \"\"\" Replace all these tokens with space to clean data reduce complications + number of words\n",
    "        Live we should do the same thing.\n",
    "        \n",
    "        Args:\n",
    "            data - a string to operate on, e.g. a sample sentence\n",
    "                or document to perform replacement upon\n",
    "        Returns:\n",
    "            modified string with symbols missing. replaced with spaces, but we will split\n",
    "            and strip the words anyway so this is fine.\n",
    "    \"\"\"\n",
    "    modified = data\n",
    "    for a in additional:\n",
    "        modified = modified.replace(a, ' ')\n",
    "    return modified\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BBC text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data path is the path to the bbc data folder\n",
    "data_path = \"data/bbc\"\n",
    "# categories of bbc data [static]\n",
    "categories = [ \n",
    "    'business',\n",
    "    'entertainment',\n",
    "    'politics',\n",
    "    'sport',\n",
    "    'tech'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "def get_file_list(category_path):\n",
    "    \"\"\" gets the list of all files in a directory\n",
    "    \"\"\"\n",
    "    return os.listdir(category_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_category(category):\n",
    "    \"\"\" \n",
    "        Loads all data from a category (for BBC dataset).\n",
    "        \n",
    "        Args:\n",
    "            category is the name of the category to be loaded\n",
    "        Returns:\n",
    "            a list of all words from all files from the category\n",
    "    \"\"\"\n",
    "    file_names = get_file_list(\"{0}/{1}/\".format(data_path, category))\n",
    "    out = []\n",
    "    for fn in file_names:\n",
    "        with open(\"{0}/{1}/{2}\".format(data_path, category, fn), 'r') as df:\n",
    "            sent = unicode(df.read(), errors='ignore')\n",
    "            sent = clean_data(sent)\n",
    "            out.extend(sent.split())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all words from all categories (and all files)\n",
    "out = [] \n",
    "for c in categories:\n",
    "    out.extend(load_category(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to rotten imdb\n",
    "data_path = 'data/rotten_imdb/'\n",
    "\n",
    "# objective\n",
    "with open(data_path + \"objective.txt\", 'r') as f:\n",
    "    objective = unicode(f.read(), errors='ignore')\n",
    "    objective = clean_data(objective)\n",
    "# subjective\n",
    "with open(data_path + \"subjective.txt\", 'r') as f:\n",
    "    subjective = unicode(f.read(), errors='ignore')\n",
    "    subjective = clean_data(subjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join data\n",
    "data = objective + subjective\n",
    "out.extend(data.split())\n",
    "words = set([ word.strip() for word in out ])\n",
    "\n",
    "# add symbols\n",
    "for a in additional:\n",
    "    words.add(a)\n",
    "words = list(set([w.lower().strip() for w in list(words)]))\n",
    "words.append('UNK')\n",
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save dictionaries\n",
    "data_path = 'data/dictionary/'\n",
    "with open(data_path+\"words.dictionary.txt\", 'w') as f:\n",
    "    for word in words:\n",
    "        f.write(\"{0}\\n\".format(word))\n",
    "with open(data_path+\"label.dictionary.txt\", 'w') as f:\n",
    "    labels = [\"objective\", \"subjective\", \"other\"]\n",
    "    for l in labels:\n",
    "        f.write(\"{0}\\n\".format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "python convert.py --input bbc\\aggregated\\bbc.test.input.txt --map dictionary\\words.dictionary.txt dictionary\\label.dictionary.txt --output bbc\\aggregated\\bbc.init.test.ctf --unk UNK\n",
    "```\n",
    "```\n",
    "python convert.py --input bbc\\aggregated\\bbc.train.input.txt --map dictionary\\words.dictionary.txt dictionary\\label.dictionary.txt --output bbc\\aggregated\\bbc.init.train.ctf --unk UNK\n",
    "```\n",
    "```\n",
    "python convert.py --input bbc\\aggregated\\bbc.val.input.txt --map dictionary\\words.dictionary.txt dictionary\\label.dictionary.txt --output bbc\\aggregated\\bbc.init.val.ctf --unk UNK\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9fa1beda41bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "print len(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
