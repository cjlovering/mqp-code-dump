{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This model is an LSTM with the following structure:\n",
    "\n",
    "```        \n",
    "                          *Recurrence*\n",
    "                      ---------------------\n",
    "IN --> Embedding --> | Bidirection LSTM    | --> Softmax\n",
    "                      ---------------------\n",
    "```\n",
    "Results:\n",
    "\n",
    "25 Epochs\n",
    "\n",
    "Train:\n",
    "     error: 0.000000\n",
    "     loss: 0.003679\n",
    "\n",
    "Validation:\n",
    "    Finished Evaluation [1]: Minibatch[1-358]: metric = 0.14% * 400;\n",
    "    \n",
    "    \n",
    "Results:\n",
    "\n",
    "With adadelta:\n",
    "\n",
    "25 epochs\n",
    "\n",
    "Train:\n",
    "    error: 0.000000\n",
    "     loss: 0.000199\n",
    "     \n",
    "Validation:\n",
    "    Finished Evaluation [1]: Minibatch[1-358]: metric = 0.00% * 400;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import cntk as C\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from cntk import Trainer, Axis\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs,\\\n",
    "        INFINITELY_REPEAT\n",
    "from cntk.learners import adam, sgd, learning_rate_schedule, UnitType, momentum_as_time_constant_schedule\n",
    "from cntk import input_variable, cross_entropy_with_softmax, \\\n",
    "        classification_error, sequence\n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.layers import Sequential, Embedding, Recurrence, LSTM, Dense, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "import math\n",
    "import numpy as np\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK componentsk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dimensions\n",
    "input_dim =  38482\n",
    "num_output_classes = 3\n",
    "features = sequence.input_variable(shape=input_dim, is_sparse=True)\n",
    "label = input_variable(num_output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the reader\n",
    "def create_reader(path, is_training, input_dim, label_dim):\n",
    "    \"\"\"\n",
    "        Creates a reader that loads data from file.\n",
    "        \n",
    "        Args:\n",
    "            path: is the relative path to the data file.\n",
    "            is_training: if reader is used for training\n",
    "            input_dim: the input dim of a word (vocab size)\n",
    "            label_dim: the number of classes\n",
    "    \"\"\"\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "        features=StreamDef(field='S0', shape=input_dim, is_sparse=True),\n",
    "        labels=StreamDef(field='S1', shape=label_dim, is_sparse=False)\n",
    "        )), randomize=is_training,\n",
    "        max_sweeps=INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "def create_lstm_v_1():\n",
    "    \"\"\"\n",
    "        Wrapper function that maintains chosen paramets.\n",
    "    \"\"\"\n",
    "    cell_dim = 50\n",
    "    hidden_dim = 150\n",
    "    embedding_dim = 100\n",
    "    return create_model(num_output_classes, embedding_dim, hidden_dim, cell_dim)\n",
    "\n",
    "\n",
    "def BiRecurrence(fwd, bwd):\n",
    "    F = C.layers.Recurrence(fwd)\n",
    "    G = C.layers.Recurrence(bwd, go_backwards=True)\n",
    "    x = C.placeholder()\n",
    "    apply_x = C.splice(F(x), G(x))\n",
    "    return apply_x\n",
    "\n",
    "\n",
    "# Defines the LSTM model for classifying sequences\n",
    "def create_model(num_output_classes, embedding_dim,\n",
    "                                hidden_dim, cell_dim):\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "            Embedding(embedding_dim, name='embed'),\n",
    "            BiRecurrence(C.layers.LSTM(hidden_dim//2), C.layers.LSTM(hidden_dim//2)),\n",
    "            sequence.last,\n",
    "            Dense(num_output_classes, name='classify')])\n",
    "    \n",
    "\n",
    "\n",
    "# Creates and trains a LSTM sequence classification model\n",
    "def train_sequence_classifier(reader, model_func, max_epochs=25):\n",
    "    \"\"\"\n",
    "        Trains a model on sequences.\n",
    "        \n",
    "        Args:\n",
    "            reader - the data source that yields data from a source file.\n",
    "            model_func - defined model instance\n",
    "            max_epochs - number of epochs to train\n",
    "        \n",
    "    \"\"\"\n",
    "    # Init model\n",
    "    model = model_func(features)\n",
    "    \n",
    "    # Constants\n",
    "    minibatch_size = 80\n",
    "    samples = 182584\n",
    "    minibatch_per_epoch = samples / minibatch_size\n",
    "    \n",
    "    # Cross Entropy and Classification Error\n",
    "    ce = cross_entropy_with_softmax(model, label)\n",
    "    pe = classification_error(model, label)\n",
    "\n",
    "    input_map = {\n",
    "        features: reader.streams.features,\n",
    "        label:    reader.streams.labels\n",
    "    }\n",
    "     \n",
    "    lr_schedule = C.learning_parameter_schedule(1, minibatch_size=C.learners.IGNORE)\n",
    "    t_schedule = C.momentum_schedule(0.971, minibatch_size=C.learners.IGNORE)\n",
    "    learner = adadelta = C.adadelta(z.parameters, lr_schedule, 0.999, 1e-6)\n",
    "    # learner = C.adam(z.parameters, lr_schedule, t_schedule, unit_gain=False)\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    progress_printer = ProgressPrinter(tag='Training', num_epochs=max_epochs, metric_is_pct=False)\n",
    "\n",
    "    trainer = Trainer(model, (ce, pe),\n",
    "                      learner, # sgd(model.parameters, lr=lr_per_sample),\n",
    "                      progress_printer)\n",
    "\n",
    "    # Get minibatches of sequences to train with and perform model training\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_end = (epoch + 1) * minibatch_per_epoch\n",
    "        while t < epoch_end:\n",
    "            mb = reader.next_minibatch(minibatch_size, input_map=input_map)\n",
    "            trainer.train_minibatch(mb)\n",
    "            t += mb[label].num_samples  \n",
    "        trainer.summarize_training_progress()\n",
    "    evaluation_average = float(trainer.previous_minibatch_evaluation_average)\n",
    "    loss_average = float(trainer.previous_minibatch_loss_average)\n",
    "    return evaluation_average, loss_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\local\\Anaconda2-5.0.0\\lib\\site-packages\\cntk\\logging\\progress_print.pyc\u001b[0m in \u001b[0;36mon_training_update_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m___write_progress_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregate_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregate_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mon_training_update_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[1;31m# Override for ProgressWriter.on_training_update_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m___generate_progress_heartbeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "SWIG director method error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-54273ccc6aff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     }\n\u001b[0;32m     35\u001b[0m ]\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mm_do_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-54273ccc6aff>\u001b[0m in \u001b[0;36mm_do_train\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_output_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_sequence_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" error: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a59d486b6af8>\u001b[0m in \u001b[0;36mtrain_sequence_classifier\u001b[1;34m(reader, model_func, max_epochs)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepoch_end\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mmb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarize_training_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda2-5.0.0\\lib\\site-packages\\cntk\\train\\trainer.pyc\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(self, arguments, outputs, device)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontains_minibatch_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 updated = super(Trainer, self).train_minibatch_overload_for_minibatchdata(\n\u001b[1;32m--> 170\u001b[1;33m                     arguments, device)\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments,\n",
      "\u001b[1;32mC:\\local\\Anaconda2-5.0.0\\lib\\site-packages\\cntk\\cntk_py.pyc\u001b[0m in \u001b[0;36mtrain_minibatch_overload_for_minibatchdata\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2803\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_minibatch_overload_for_minibatchdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2804\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer_train_minibatch_overload_for_minibatchdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2806\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: SWIG director method error."
     ]
    }
   ],
   "source": [
    "def do_train():\n",
    "    \"\"\"\n",
    "        Wrapper function for training the network.\n",
    "    \"\"\"\n",
    "    global z\n",
    "    path = (\"data/rotten_imdb/imdb.train.ctf\")\n",
    "    reader = create_reader(path, True, input_dim, num_output_classes)\n",
    "    z = create_lstm_v_1()\n",
    "    error, loss = train_sequence_classifier(reader, z)\n",
    "    print(\" error: %f\" % error)\n",
    "    print(\" loss: %f\" % loss)\n",
    "\n",
    "def m_do_train(config):\n",
    "    \"\"\"\n",
    "        Wrapper function for training the network.\n",
    "    \"\"\"\n",
    "    global z\n",
    "    z = create_lstm_v_1()\n",
    "    \n",
    "    for c in config:\n",
    "        path = c['path']\n",
    "        iterations = c['iterations']\n",
    "    \n",
    "        reader = create_reader(path, True, input_dim, num_output_classes)\n",
    "        error, loss = train_sequence_classifier(reader, z, iterations)\n",
    "\n",
    "        print(\" error: %f\" % error)\n",
    "        print(\" loss: %f\" % loss)\n",
    "    return z\n",
    "config = [\n",
    "    {\n",
    "        \"path\": \"data/rotten_imdb/imdb.train.ctf\",\n",
    "        \"iterations\": 25\n",
    "    }\n",
    "]\n",
    "m_do_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function_preferred(model, labels):\n",
    "    \"\"\"\n",
    "        Creates a tuple of criterions, softmax and classification error\n",
    "        \n",
    "        Args:\n",
    "            model - that is being trained.\n",
    "            labels - labels to compare with (cntk Variable)\n",
    "        Returns:\n",
    "            Tuple of functions: (softmax, classification)\n",
    "    \"\"\"\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)\n",
    "\n",
    "def evaluate(reader, model_func):\n",
    "    \"\"\"\n",
    "        Evaluates the model given a reader (data source) and\n",
    "        the trained model.\n",
    "        \n",
    "        Args:\n",
    "            reader - data source that reads from files.\n",
    "            model_func - the trained model.\n",
    "    \"\"\"\n",
    "    # Instantiate the model function; x is the input (feature) variable\n",
    "    model = model_func(features)\n",
    "\n",
    "    # Create the loss and error functions\n",
    "    loss, label_error = create_criterion_function_preferred(model, label)\n",
    "\n",
    "    # process minibatches and perform evaluation\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Evaluation', num_epochs=0)\n",
    "\n",
    "    while True:\n",
    "        minibatch_size = 32\n",
    "        data = reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            features: reader.streams.features,\n",
    "            label: reader.streams.labels\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "\n",
    "        evaluator = C.eval.Evaluator(loss, progress_printer)\n",
    "        evaluator.test_minibatch(data)\n",
    "\n",
    "    evaluator.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test(path):\n",
    "    \"\"\" Tests the network on the validation set. \"\"\"\n",
    "    input_dim =  38482\n",
    "    num_output_classes = 3\n",
    "    cell_dim = 50\n",
    "    hidden_dim = 150\n",
    "    embedding_dim = 100\n",
    "\n",
    "    reader = create_reader(path, False, input_dim, num_output_classes)\n",
    "    evaluate(reader, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 1.0\n",
      "Finished Epoch[1 of 25]: [Training] loss = 0.709728 * 2282, metric = 0.464505 * 2282 16.882s (135.2 samples/s);\n",
      "Finished Epoch[2 of 25]: [Training] loss = 0.570168 * 2284, metric = 0.278459 * 2284 16.169s (141.3 samples/s);\n",
      "Finished Epoch[3 of 25]: [Training] loss = 0.353434 * 2280, metric = 0.143860 * 2280 14.595s (156.2 samples/s);\n",
      "Finished Epoch[4 of 25]: [Training] loss = 0.239553 * 2284, metric = 0.092382 * 2284 14.523s (157.3 samples/s);\n",
      "Finished Epoch[5 of 25]: [Training] loss = 0.164662 * 2281, metric = 0.056993 * 2281 14.900s (153.1 samples/s);\n",
      "Finished Epoch[6 of 25]: [Training] loss = 0.179889 * 2282, metric = 0.065294 * 2282 14.839s (153.8 samples/s);\n",
      "Finished Epoch[7 of 25]: [Training] loss = 0.135258 * 2281, metric = 0.046909 * 2281 14.952s (152.6 samples/s);\n",
      "Finished Epoch[8 of 25]: [Training] loss = 0.049729 * 2283, metric = 0.013579 * 2283 14.762s (154.7 samples/s);\n",
      "Finished Epoch[9 of 25]: [Training] loss = 0.038919 * 2282, metric = 0.010955 * 2282 14.603s (156.3 samples/s);\n",
      "Finished Epoch[10 of 25]: [Training] loss = 0.054409 * 2281, metric = 0.017975 * 2281 14.674s (155.4 samples/s);\n",
      "Finished Epoch[11 of 25]: [Training] loss = 0.015363 * 2282, metric = 0.004382 * 2282 14.834s (153.8 samples/s);\n",
      "Finished Epoch[12 of 25]: [Training] loss = 0.023871 * 2282, metric = 0.007011 * 2282 15.151s (150.6 samples/s);\n",
      "Finished Epoch[13 of 25]: [Training] loss = 0.019949 * 2282, metric = 0.004820 * 2282 14.651s (155.8 samples/s);\n",
      "Finished Epoch[14 of 25]: [Training] loss = 0.011940 * 2282, metric = 0.002629 * 2282 14.325s (159.3 samples/s);\n",
      "Finished Epoch[15 of 25]: [Training] loss = 0.003027 * 2284, metric = 0.000438 * 2284 15.372s (148.6 samples/s);\n",
      "Finished Epoch[16 of 25]: [Training] loss = 0.009745 * 2282, metric = 0.003067 * 2282 18.339s (124.4 samples/s);\n",
      "Finished Epoch[17 of 25]: [Training] loss = 0.008424 * 2281, metric = 0.001315 * 2281 18.598s (122.6 samples/s);\n",
      "Finished Epoch[18 of 25]: [Training] loss = 0.001117 * 2283, metric = 0.000438 * 2283 16.896s (135.1 samples/s);\n",
      "Finished Epoch[19 of 25]: [Training] loss = 0.002419 * 2282, metric = 0.001315 * 2282 14.994s (152.2 samples/s);\n",
      "Finished Epoch[20 of 25]: [Training] loss = 0.010293 * 2281, metric = 0.001754 * 2281 15.089s (151.2 samples/s);\n",
      "Finished Epoch[21 of 25]: [Training] loss = 0.001944 * 2283, metric = 0.000438 * 2283 15.014s (152.1 samples/s);\n",
      "Finished Epoch[22 of 25]: [Training] loss = 0.000892 * 2281, metric = 0.000000 * 2281 14.588s (156.4 samples/s);\n",
      "Finished Epoch[23 of 25]: [Training] loss = 0.003866 * 2281, metric = 0.000877 * 2281 14.847s (153.6 samples/s);\n",
      "Finished Epoch[24 of 25]: [Training] loss = 0.001181 * 2282, metric = 0.000438 * 2282 15.361s (148.6 samples/s);\n",
      "Finished Epoch[25 of 25]: [Training] loss = 0.002769 * 2286, metric = 0.000875 * 2286 14.909s (153.3 samples/s);\n",
      " error: 0.000000\n",
      " loss: 0.000222\n",
      "Learning rate per minibatch: 1.0\n",
      "Finished Epoch[1 of 5]: [Training] loss = 0.115300 * 2284, metric = 0.025832 * 2284 12.473s (183.1 samples/s);\n",
      "Finished Epoch[2 of 5]: [Training] loss = 0.000905 * 2281, metric = 0.000000 * 2281 12.579s (181.3 samples/s);\n",
      "Finished Epoch[3 of 5]: [Training] loss = 0.000522 * 2281, metric = 0.000000 * 2281 12.525s (182.1 samples/s);\n",
      "Finished Epoch[4 of 5]: [Training] loss = 0.000365 * 2285, metric = 0.000000 * 2285 12.512s (182.6 samples/s);\n",
      "Finished Epoch[5 of 5]: [Training] loss = 0.000282 * 2282, metric = 0.000000 * 2282 12.421s (183.7 samples/s);\n",
      " error: 0.000000\n",
      " loss: 0.000615\n",
      "Finished Evaluation [1]: Minibatch[1-358]: metric = 0.00% * 400;\n",
      "Finished Evaluation [1]: Minibatch[1-6]: metric = 0.00% * 7;\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    {\n",
    "        \"path\": \"data/rotten_imdb/imdb.train.ctf\",\n",
    "        \"iterations\": 25\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"data/bbc/aggregated/bbc.250.init.train.ctf\",\n",
    "        \"iterations\": 5\n",
    "    }\n",
    "]\n",
    "\n",
    "z = m_do_train(config)\n",
    "\n",
    "do_test(\"data/rotten_imdb/imdb.val.ctf\")\n",
    "do_test(\"data/bbc/aggregated/bbc.250.init.val.ctf\")\n",
    "do_test(\"data/bbc/aggregated/bbc.250.init.test.ctf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-104]: metric = 0.00% * 124;\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z.save(\"imbd.v3-1.cntk.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cntk import load_model\n",
    "loaded_model = load_model(\"imbd.v3-1.cntk.mdl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
